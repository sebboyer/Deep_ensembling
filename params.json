{
  "name": "Deep ensembling",
  "tagline": "A model for ensembling models from diverse datasets",
  "body": "## What is Deep_ensembling ?.\r\nDeep_ensembling is a package that let you train models on different data-sets and combine their prediction to build a robust predictive model. This package is intended to prediction problems meeting the following criteria :\r\n- Several full data sources (both features and outcomes available) available\r\n- A binary classification problem\r\nThis package let you define easily what kind of classification model to build and how to combine them. The outcome of the workflow is a robust predictive model that is likely to perform very well on a new data source.\r\n\r\n## How to use it ?\r\n### Training ensemble models\r\nThe script training.py allows you to train multiple binary models on multiple training Datasets in parallel.\r\nThe following lines will train a LogisticRegression, a NearestNeighbor and a RandomForest classifier on the list of dataset provided in X_train_list and y_train_list (each of the data in the list X_train_list must share the same second dimension) :\r\n\r\n```python\r\nimport training as train\r\n\r\nmodel_list = ['lr','rf','nn']  # The models you want to train ('lr','rf','nn' or 'svm')\r\nparams_list = [(1,4),(0,0),(80,150)] # The associated parameters range\r\noutput_filename = 'TrainedModels.p' # The name of the output files where estimators will be saved\r\ntrain.main(X_train_list,y_train_list,model_list,params_list,output_filename=output_filename) # Train !\r\n```\r\n### Creating Ensembles\r\nThis package allows you to create your own from very simple to very complex \"Ensembling method\". \r\n\r\nA structure is defined as an object from class 'Network'. When no 'links' between layers are mentionned the default is fully-connected. We show code to create the two structures shown in the figure below.\r\n\r\n```python\r\nfrom ensembling import *\r\n\r\n#### Structure A \r\nN=Network() # Define Network\r\nN.add_layer(\"Models_layer\",[]) # First layer\r\nN.add_layer(\"Output_layer\",[Vote(\"simple\")])  # Last Layer\r\n\r\n#### Structure B \r\nN=Network() # Define Network\r\nN.add_layer(\"Models_layer\",[]) # First layer\r\n\r\nn_input_models_per_source = 3 # Second layer\r\nn_sources = 3\r\nn_output_models_per_source = 2\r\nN.add_layer(\"Hidden_layer\",[Vote(\"simple\"),Vote(\"norm\")])\r\nN.layers[1].links = create_independent_links(n_input_models_per_source,n_sources,n_output_models_per_source)\r\n\r\nN.add_layer(\"Hidden_layer\",[Vote(\"simple\"),Vote(\"rank\"),Vote(\"norm\")]) # Third Layer\r\nN.add_layer(\"Output_layer\",[Vote(\"simple\")])  # Fourth Layer\r\n```\r\n\r\n### Testing ensemble models\r\n\r\nThe script testing.py allows you to test you trained models. More importantly it allows you to have them vote in a structured way that you can define. We provide a default structure :\r\n\r\nThe following lines will test the AUC of the 'network' with the 'estimators' on X_test,y_test (X_train and y_train are used only if the network involve 'meta-model' or 'stacking' and therefore need to be trained).\r\n\r\n```python\r\nfrom testing import *\r\n\r\nens = Ensemble(estimators,network_file) # Define Ensembling method\r\nplot_graph(net) # Plot ensembling structure\r\n\r\nens.train(X_val,y_val) # Train the meta models on some validation data\r\nres = ens.eval(X_test,y_test) # Evaluate each estimator on each layer of the ensembling structure\r\n\r\nplot_graph(net,results = res) # Plot ensembling results\r\n```\r\n\r\n### Authors and Contributors\r\nSebastien Boyer, sebboyer@csail.mit.edu\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}